I want:

1. Create a new ros2 package in this workspace called "can_do_challenge", similar to perimeter_roamer_v3. I'm going to want to you create a behavior tree called "main.xml" following instruction below.

2. Flesh out as much code as can. Read the following first as I want you to create an initial behavior tree and supporting code for this challenge.

4. I'd like to be able to run some sort of simulation of the behavior tree, so create a launch file and other artifacts as needed. It will be a simulation that uses Gazebo. You can look at the alias 'sim' to see how I launch Sigyn today in simulation. The existing simulation is missing features needed for the challenge though--in particular it is missing a table with a can of CokeZero on it. So just create a clone of the world file used by the 'sim' alias that has added a table and a can of CokeZero on it. The can can be a simple cylinder with a texture on it. The table can be a simple box or a kitchen table.

Here's some background.

The challenge is to start whereever the robot is current located, assumed to be next to me. The robot then is to find a can of CokeZero, pick it up and return it to the starting pose (i.e., next to me). The can, for now, will be sitting near the edge of the table top.

The robot is, of course, SIgyn. Sigyn has, especially for this challenge
* An OAK-D camera nearly 4 meters above the top plate of Sigyn pointed downwards towards the front of the robot. It can see the retracted gripper and maybe a meter in front of that. It is attached near the top of the gripper elevator. The Oak-D is used for localization and object recognition. It is intended to be able to see the CokeZero can on a table some distance away. The robot is intended to use the expected location of the CokeZero can to help move itself within a couple of feet of the expected location of the Coke can, then see the can itself in the OAK-D and navigate within about a foot of it, positioning itself within the 18 inch reach of the gripper and rotated somewhat so the gripper can reach the can with a bit finer positioning using the Raspberry Pi camera.
* Two LIDARs, a top_lidar located a bit higher than the OAK-D at the very top of the elevator and is used for localization, and a cup_lidar located just above the base. THe cup_lidar is locate where the top plate is on Sigyn and is used for obstacle avoidance.
* The gripper consists of a 4 meter elevator controlled by a stepper motor, an 18 inch extender attached at a right angle to the elevator, and a gripper on the end of the extender which can open and close. To grasp the can of CokeZero, the elevator is raised to just above the table height, the extender is extended as needed, and the gripper is closed around the can. There is no force sensor so the code to close the gripper will estimate how much to close the gripper based on the known diameter of the can of CokeZero.
* When traveling to the can and returning, the gripper mechhanism should be retracted and lowered to minimize the chance of hitting something. After the can is grasped, the extender should retrace and lower itself to just above the bottom of the elevator. Not to the actual bottom, since that might be too low and cause the can to hit the top plate and fall out of the gripper.
* There is a Raspberry Pi 5 computer with an AI Hat attached and a Pi Camera v3 connected to the AI Hat. The camera is located a few inches behind the gripper and slightly above it so it can see what the gripper is doing. The camera is intended to do object recognition to help position the gripper so that the CokeZero can be grasped and verify that it has been grasped. 
* When the OAK-D has been used to position the robot vaguely within reach of the can, the elevator should be raised until the can should be visible in the Pi camera. This is done by using the z-height estimate of the bottom of the Coke can from the OAKD-D camera.
* The Pi camera should be able to see the Coke but the can will likely not be centered in the image. The robot should use the Pi camera to center the can in the image by moving the gripper left/right and forward/backward using the extender. Once centered, the gripper should lower itself to just above the table and then close the gripper around the can.
* Look at sigyn_to_sensor_v2 to see the commands that can command the gripper. There is an odom topic and a map to odom transform. Object coordinates will be defined in the map frame. sigyn_to_sensor_v2 also publishes topics of interest, like battery state, estop state, tilt state, etc. (well, the IMU can be intepreted to get tilt state). You will use the nav2 actions, services and topics just like perimeter_roamer_v3 does to move the robot around.
* The bluetooth_joystick package shows how I use a joystick to teleoperate the robot, including manipulating the gripper. I will need to create a packages (not yet) that provide object recognition using the OAK-D and Pi cameras. For now, you can create placeholder code for those parts.

Eventually, the robot will have a database, a simple json list for now, of known, approximate locations where the can of CokeZero might be. For now, just assume there is one known location. The robot should use this location to help it localize itself in the environment using the OAK-D camera to see the Coke can (or not) when it nears the expected can location, then search for the can of CokeZero from there and grasp the Coke can as described above. Once grasped, the robot should return to the starting location . That's enough for now.

So, I want you to create the package, create an initial behavior tree. You can use FMAB1.xml for inspiration, along with how I began improvements in a2.xml. I will work on the behavior tree with iterations to its structure. See if you can generate code to launch a simulations, using sim.launch.py as your inspireation, using a clone of the world file used by the sim alias but with a table and a can of CokeZero on it. You can create simple placeholder code for the various actions and conditions needed for the behavior tree. I will fill in the details later. Just make sure the behavior tree can be run in simulation. Create a main.xml behavior tree as a starting point.

It's a big ask. We can do it in phases.

Ask any questions if you have them.